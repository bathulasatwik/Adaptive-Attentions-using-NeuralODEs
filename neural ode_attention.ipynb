{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abed543-c42f-4984-96e8-e813b81462dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdcc7b35-f287-43c1-84f2-47088ef5a490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.1/44.1 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.0 MB 7.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/10.0 MB 7.4 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/10.0 MB 18.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.8/10.0 MB 27.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/10.0 MB 42.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.0/10.0 MB 45.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 40.1 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 286.3/286.3 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.3-cp312-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.4/2.4 MB 50.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 50.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.4.5 tokenizers-0.20.3 transformers-4.46.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d03b14-47cf-4f9d-81bd-3a3bc249dbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ODEBertSelfAttention...\n",
      "Context Layer Shape: torch.Size([4, 128, 768])\n",
      "Attention Weights Shape: torch.Size([48, 128, 128])\n",
      "\n",
      "Testing ODEBertModel...\n",
      "Last Hidden State Shape: torch.Size([4, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertConfig\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "\n",
    "class AttentionODEFunc(nn.Module):\n",
    "    def __init__(self, head_dim):\n",
    "        super(AttentionODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(head_dim, head_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_dim, head_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, attention_scores):\n",
    "        return self.net(attention_scores)\n",
    "\n",
    "\n",
    "class ODEAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=8, time_steps=10):\n",
    "        super(ODEAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        self.time_steps = time_steps\n",
    "        self.scaling_factor = self.head_dim ** 0.5\n",
    "\n",
    "        self.ode_func = AttentionODEFunc(self.head_dim)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        batch_size, seq_length, _ = Q.size()\n",
    "        assert Q.size(-1) == self.hidden_dim, f\"Expected hidden size {self.hidden_dim}, got {Q.size(-1)}\"\n",
    "\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        Q = Q.reshape(-1, seq_length, self.head_dim)\n",
    "        K = K.reshape(-1, seq_length, self.head_dim)\n",
    "        V = V.reshape(-1, seq_length, self.head_dim)\n",
    "\n",
    "        initial_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scaling_factor\n",
    "\n",
    "        initial_scores = initial_scores.unsqueeze(-1)\n",
    "\n",
    "        initial_scores = initial_scores.view(-1, self.head_dim)\n",
    "\n",
    "        t_span = torch.linspace(0, 1, steps=self.time_steps).to(Q.device)\n",
    "\n",
    "        evolved_scores = odeint(self.ode_func, initial_scores, t_span)\n",
    "\n",
    "        evolved_scores = evolved_scores[-1].view(-1, seq_length, seq_length)\n",
    "\n",
    "        batch_heads = evolved_scores.size(0)\n",
    "        expected_size = batch_heads * seq_length * seq_length\n",
    "        assert evolved_scores.numel() == expected_size, (\n",
    "            f\"Shape mismatch: expected {expected_size}, got {evolved_scores.numel()}\"\n",
    "        )\n",
    "\n",
    "        attention_weights = torch.softmax(evolved_scores, dim=-1)\n",
    "\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        output = output.view(batch_size, self.num_heads, seq_length, self.head_dim).permute(0, 2, 1, 3)\n",
    "        output = output.reshape(batch_size, seq_length, self.hidden_dim)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "class ODEBertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ODEBertSelfAttention, self).__init__()\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.ode_attention = ODEAttention(config.hidden_size, config.num_attention_heads)\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        Q = self.query(hidden_states)\n",
    "        K = self.key(hidden_states)\n",
    "        V = self.value(hidden_states)\n",
    "        context_layer, attention_weights = self.ode_attention(Q, K, V)\n",
    "        context_layer = self.dense(context_layer)\n",
    "        context_layer = self.dropout(context_layer)\n",
    "        return context_layer, attention_weights\n",
    "\n",
    "\n",
    "class ODEBertModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ODEBertModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.encoder = nn.ModuleList(\n",
    "            [ODEBertSelfAttention(config) for _ in range(config.num_hidden_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        embedded_inputs = self.embeddings(input_ids)\n",
    "        hidden_states = embedded_inputs\n",
    "        for layer in self.encoder:\n",
    "            hidden_states, _ = layer(hidden_states)\n",
    "        return {\"last_hidden_state\": hidden_states}\n",
    "\n",
    "\n",
    "def test_self_attention():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config = BertConfig(hidden_size=768, num_attention_heads=12, attention_probs_dropout_prob=0.1)\n",
    "    ode_self_attention = ODEBertSelfAttention(config).to(device)\n",
    "\n",
    "    # Synthetic input\n",
    "    batch_size = 4\n",
    "    seq_length = 128\n",
    "    hidden_states = torch.rand(batch_size, seq_length, config.hidden_size).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    context_layer, attention_weights = ode_self_attention(hidden_states)\n",
    "    print(f\"Context Layer Shape: {context_layer.shape}\")\n",
    "    print(f\"Attention Weights Shape: {attention_weights.shape}\")\n",
    "\n",
    "\n",
    "def test_bert_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config = BertConfig(\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=4,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "    )\n",
    "    ode_bert_model = ODEBertModel(config).to(device)\n",
    "\n",
    "    batch_size = 4\n",
    "    seq_length = 128\n",
    "    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length)).to(device)\n",
    "    attention_mask = torch.ones((batch_size, seq_length)).to(device)\n",
    "\n",
    "    outputs = ode_bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    print(f\"Last Hidden State Shape: {outputs['last_hidden_state'].shape}\")\n",
    "\n",
    "\n",
    "# testinggg\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing ODEBertSelfAttention...\")\n",
    "    test_self_attention()\n",
    "    print(\"\\nTesting ODEBertModel...\")\n",
    "    test_bert_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d9c1ff6-1a23-4fda-ba66-2dfbc282c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEBertForTextClassification(ODEBertModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs[\"last_hidden_state\"][:, 0, :]\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        logits = self.classifier(cls_output)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            loss = criterion(logits, labels)\n",
    "        return {\"loss\": loss, \"logits\": logits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeac4bf9-2b4b-46bf-a4fc-4e2f412e4518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing ODEBertForTextClassification on GPU...\n",
      "Loss: 0.6983002424240112\n",
      "Logits Shape: torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "def test_text_classification():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    config = BertConfig(\n",
    "        vocab_size=30522,\n",
    "        hidden_size=768,\n",
    "        num_attention_heads=12,\n",
    "        num_hidden_layers=4,\n",
    "        attention_probs_dropout_prob=0.1,\n",
    "        hidden_dropout_prob=0.1,\n",
    "    )\n",
    "    num_labels = 2  \n",
    "\n",
    "    model = ODEBertForTextClassification(config, num_labels).to(device)\n",
    "\n",
    "    batch_size = 4\n",
    "    seq_length = 128\n",
    "    input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_length)).to(device)\n",
    "    attention_mask = torch.ones((batch_size, seq_length)).to(device)\n",
    "    labels = torch.randint(0, num_labels, (batch_size,)).to(device)  \n",
    "\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    print(f\"Loss: {outputs['loss']}\")\n",
    "    print(f\"Logits Shape: {outputs['logits'].shape}\") \n",
    "\n",
    "\n",
    "#  testing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing ODEBertForTextClassification on GPU...\")\n",
    "    test_text_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e615c050-a2bd-4de1-8b33-b136fb6dca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2f4d71e-b0cb-4740-a081-42efaef326e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b763f562-cc33-45cb-bca9-5a3f3a51e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2  \n",
    "model = ODEBertForTextClassification(config, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c31d77b2-5c24-4a6b-b9bd-7cece9912651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4bcbc51ad248d3b6698b0531a2ccbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cebd585b6c54961abff48313cf0e76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee8f7f41d124080a5c9e3649ea689f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dd504a01f24f79ba6ad9ee2b0678d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa59168ecb9d47e5919bfa37a2377e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1967e4ee43d2478f9756f8efc9aceff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4035f94f5eef47ba84a3aa45a3158cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93f48e9b-6242-4a70-8d8e-24237faec83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9c4f2fd1dc4ff2b3aca6b9fd066a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1648d749dc824a49b34e83a4bfcd79ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddf011e217144809b3f8336307586b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db7a414e-6e01-4097-a880-16c304f480bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da26aab-4cb5-4299-b4fb-73f610fb9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(200))\n",
    "test_data = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(50))\n",
    "train_dataloader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69d05d4e-1cbb-46cf-b370-281be46afb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fd9a5d0-763c-44ea-863a-21fca8d3188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████████████████████████████████████████████████| 25/25 [20:58<00:00, 50.35s/batch, loss=0.697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.6947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████████████████████████████████████████████████| 25/25 [20:35<00:00, 49.43s/batch, loss=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Loss: 0.6922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████████████████████████████████████████████████| 25/25 [20:21<00:00, 48.85s/batch, loss=0.685]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Loss: 0.7855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|███████████████████████████████████████████████████████| 25/25 [20:51<00:00, 50.06s/batch, loss=0.66]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Loss: 0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████████████████████████████████████████████████| 25/25 [20:36<00:00, 49.45s/batch, loss=0.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Loss: 0.6814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████████████████████████████████████████████████| 25/25 [20:47<00:00, 49.91s/batch, loss=0.771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Loss: 0.6756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████████████████████████████████████████████████| 25/25 [22:15<00:00, 53.42s/batch, loss=0.752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Loss: 0.6682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████████████████████████████████████████████████| 25/25 [21:24<00:00, 51.37s/batch, loss=0.541]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Average Loss: 0.6411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████████████████████████████████████████████████| 25/25 [21:40<00:00, 52.02s/batch, loss=0.675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Average Loss: 0.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████████████████████████████████████████████████| 25/25 [21:27<00:00, 51.51s/batch, loss=1.13]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Average Loss: 0.5383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|█████████████████████████████████████████████████████| 25/25 [21:06<00:00, 50.66s/batch, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Average Loss: 0.4951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|█████████████████████████████████████████████████████| 25/25 [19:37<00:00, 47.09s/batch, loss=0.566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Average Loss: 0.2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|█████████████████████████████████████████████████████| 25/25 [19:31<00:00, 46.86s/batch, loss=0.559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Average Loss: 0.2413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|█████████████████████████████████████████████████████| 25/25 [19:35<00:00, 47.03s/batch, loss=0.932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Average Loss: 0.3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|███████████████████████████████████████████████████| 25/25 [18:55<00:00, 45.41s/batch, loss=0.00507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Average Loss: 0.1321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|████████████████████████████████████████████████████| 25/25 [19:23<00:00, 46.56s/batch, loss=0.0096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Average Loss: 0.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100:   0%|                                                                          | 0/25 [00:29<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m], labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\function.py:291\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;124;03m    This class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    292\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m        Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Wrap the dataloader with tqdm for progress tracking\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], labels=batch[\"labels\"])\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update the progress bar with the current loss\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0045c60-070f-413b-b89f-6bed97fc40d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
